name: backfill-month

on:
  workflow_dispatch:
    inputs:
      month:
        description: 'Month to backfill (YYYYMM)'
        required: true
      no_latest:
        description: 'Do not update selections/latest.json (default false)'
        required: false
        default: 'false'
      workers:
        description: 'Parallel translation workers'
        required: false
        default: '20'
      deploy:
        description: 'Deploy site after backfill (true/false)'
        required: false
        default: 'true'

jobs:
  backfill:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - name: Capture run metadata
        id: meta
        run: echo "started_at=$(date -u +%Y-%m-%dT%H:%M:%SZ)" >> $GITHUB_OUTPUT
      - name: Set Backblaze region (job-wide)
        run: echo "AWS_DEFAULT_REGION=us-west-004" >> $GITHUB_ENV
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
          persist-credentials: true
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      - name: Install pandoc for PDF generation
        run: |
          sudo apt-get update
          sudo apt-get install -y pandoc texlive-latex-base texlive-fonts-recommended
          which pandoc || { echo "‚ùå pandoc installation failed"; exit 1; }
          pandoc --version | head -1
      - name: "Preflight: Validate OpenRouter API key"
        env:
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
        run: |
          set -e
          echo "üîé Preflight check: validating OPENROUTER_API_KEY"
          python -m src.tools.env_diagnose --check || true
          python -m src.tools.env_diagnose --validate || {
            echo "‚ùå OPENROUTER_API_KEY invalid or missing ‚Äî failing early"
            exit 1
          }
      - name: Backfill month
        env:
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
          DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
          BRIGHTDATA_API_KEY: ${{ secrets.BRIGHTDATA_API_KEY }}
          BRIGHTDATA_ZONE: ${{ secrets.BRIGHTDATA_ZONE }}
          BACKBLAZE_KEY_ID: ${{ secrets.BACKBLAZE_KEY_ID }}
          BACKBLAZE_APPLICATION_KEY: ${{ secrets.BACKBLAZE_APPLICATION_KEY }}
          BACKBLAZE_S3_ENDPOINT: ${{ secrets.BACKBLAZE_S3_ENDPOINT }}
          BACKBLAZE_BUCKET: ${{ secrets.BACKBLAZE_BUCKET }}
          BACKBLAZE_PREFIX: ${{ secrets.BACKBLAZE_PREFIX }}
        run: |
          set -e
          echo "üöÄ Backfill for month: ${{ inputs.month }}"
          
          REC="data/records/chinaxiv_${{ inputs.month }}.json"
          if [ -f "$REC" ]; then
            echo "üìÑ Found existing records: $REC ‚Äî skipping harvest"
          else
            if [ -z "${BRIGHTDATA_API_KEY}" ] || [ -z "${BRIGHTDATA_ZONE}" ]; then
              echo "‚ùå BrightData credentials are required for backfill (no existing $REC)"
              exit 1
            fi
            echo "üì• Harvesting (optimized) for ${{ inputs.month }}..."
            python -m src.harvest_chinaxiv_optimized --month "${{ inputs.month }}" || {
              echo "‚ùå Harvest failed"
              exit 1
            }
            if [ ! -f "$REC" ]; then
              echo "‚ùå Records not found after harvest: $REC"
              exit 1
            fi
          fi
          
          echo "üìã Selecting new items from $REC..."
          python -m src.select_and_fetch --records "$REC" --output data/selected.json || {
            echo "‚ùå Selection failed"
            exit 1
          }

          echo "üß≠ Persisting selection to B2 and re-reading from B2..."
          if [ -n "${BACKBLAZE_KEY_ID}" ] && [ -n "${BACKBLAZE_APPLICATION_KEY}" ] && [ -n "${BACKBLAZE_S3_ENDPOINT}" ] && [ -n "${BACKBLAZE_BUCKET}" ]; then
            python -m pip install --upgrade pip >/dev/null 2>&1 || true
            pip install awscli >/dev/null 2>&1 || true
            export AWS_ACCESS_KEY_ID="${BACKBLAZE_KEY_ID}"
            export AWS_SECRET_ACCESS_KEY="${BACKBLAZE_APPLICATION_KEY}"
            export AWS_DEFAULT_REGION=us-west-004
            DEST="s3://${BACKBLAZE_BUCKET}/${BACKBLAZE_PREFIX}"
            DAY=$(date -u +"%Y-%m-%d")
            SELECT_KEY="selections/daily/${DAY}.json"
            aws s3 cp data/selected.json "${DEST}${SELECT_KEY}" --endpoint-url "${BACKBLAZE_S3_ENDPOINT}" || {
              echo "‚ùå Failed to upload selection to B2";
              python -m src.tools.b2_alerts add "upload selection failed: ${SELECT_KEY}";
              exit 1;
            }
            if [ "${{ inputs.no_latest }}" != "true" ]; then
              echo "{\"path\": \"${SELECT_KEY}\", \"run_id\": \"${GITHUB_RUN_ID}\", \"ts\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\"}" > latest.json
              aws s3 cp latest.json "${DEST}selections/latest.json" --endpoint-url "${BACKBLAZE_S3_ENDPOINT}" || {
                echo "‚ùå Failed to update selections/latest.json";
                python -m src.tools.b2_alerts add "update latest.json failed";
                exit 1;
              }
            fi
            rm -f data/selected.json
            aws s3 cp "${DEST}${SELECT_KEY}" data/selected.json --endpoint-url "${BACKBLAZE_S3_ENDPOINT}" || {
              echo "‚ùå Failed to pull selection from B2";
              python -m src.tools.b2_alerts add "download selection failed: ${SELECT_KEY}";
              exit 1;
            }
            export SELECT_KEY
            echo "SELECT_KEY=${SELECT_KEY}" >> $GITHUB_ENV
          else
            echo "‚ùå B2 secrets missing; cannot persist selection"
            python -m src.tools.b2_alerts add "B2 selection persist skipped: missing secrets"
            exit 1
          fi
          
          # MANDATORY: Download PDFs before translation
          echo "üì• Downloading PDFs for month ${{ inputs.month }}..."
          python scripts/download_missing_pdfs.py --months "${{ inputs.month }}" || {
            echo "‚ö†Ô∏è Some PDF downloads failed, continuing with available PDFs"
          }

          # Upload PDFs to B2 BEFORE translation (ensures persistence)
          if [ -d data/pdfs ] && ls data/pdfs/chinaxiv-${{ inputs.month }}*.pdf 1>/dev/null 2>&1; then
            echo "üì§ Uploading PDFs to B2 before translation..."
            aws s3 sync data/pdfs "${DEST}pdfs/" \
              --exclude "*" \
              --include "chinaxiv-${{ inputs.month }}*.pdf" \
              --endpoint-url "${BACKBLAZE_S3_ENDPOINT}" \
              --only-show-errors
            PDF_COUNT=$(ls -1 data/pdfs/chinaxiv-${{ inputs.month }}*.pdf 2>/dev/null | wc -l | tr -d ' ')
            echo "‚úÖ ${PDF_COUNT} PDFs synced to B2"
          else
            echo "‚ö†Ô∏è No PDFs found for month ${{ inputs.month }}"
          fi

          echo "üåê Translating (workers=${{ inputs.workers }})..."
          python -m src.pipeline --skip-selection --workers "${{ inputs.workers }}" --with-qa || {
            echo "‚ùå Pipeline failed"
            exit 1
          }
          echo "‚úÖ Backfill and site build completed for ${{ inputs.month }}"
      - name: Publish validated/flagged/PDFs to B2 and write manifests
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.BACKBLAZE_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.BACKBLAZE_APPLICATION_KEY }}
          B2_S3_ENDPOINT: ${{ secrets.BACKBLAZE_S3_ENDPOINT }}
          B2_BUCKET: ${{ secrets.BACKBLAZE_BUCKET }}
          B2_PREFIX: ${{ secrets.BACKBLAZE_PREFIX }}
          SELECT_KEY: ${{ env.SELECT_KEY }}
          RECORDS_KEYS: records/chinaxiv_${{ inputs.month }}.json
          RUN_STARTED_AT: ${{ steps.meta.outputs.started_at }}
          B2_FAIL_ON_ERROR: 'true'
        run: |
          # Ensure AWS CLI signs requests for Backblaze region
          export AWS_DEFAULT_REGION=us-west-004
          python -m pip install --upgrade pip >/dev/null 2>&1 || true
          pip install awscli >/dev/null 2>&1 || true
          python -m src.tools.b2_publish || {
            echo "‚ùå B2 publish failed";
            exit 1;
          }

      - name: Hydrate validated translations from B2 (pre-render)
        if: ${{ inputs.deploy == 'true' }}
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.BACKBLAZE_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.BACKBLAZE_APPLICATION_KEY }}
          BACKBLAZE_S3_ENDPOINT: ${{ secrets.BACKBLAZE_S3_ENDPOINT }}
          BACKBLAZE_BUCKET: ${{ secrets.BACKBLAZE_BUCKET }}
          BACKBLAZE_PREFIX: ${{ secrets.BACKBLAZE_PREFIX }}
        run: |
          set -e
          if [ -z "${AWS_ACCESS_KEY_ID}" ] || [ -z "${AWS_SECRET_ACCESS_KEY}" ] || [ -z "${BACKBLAZE_S3_ENDPOINT}" ] || [ -z "${BACKBLAZE_BUCKET}" ]; then
            echo "‚ùå Missing B2 credentials for hydration";
            python -m src.tools.b2_alerts add "hydrate skipped: missing B2 credentials";
            exit 1;
          fi
          python -m pip install --upgrade pip >/dev/null 2>&1 || true
          pip install awscli >/dev/null 2>&1 || true
          export AWS_DEFAULT_REGION=us-west-004
          DEST="s3://${BACKBLAZE_BUCKET}/${BACKBLAZE_PREFIX}"
          echo "üßØ Reset local translations (data/translated)"
          rm -rf data/translated && mkdir -p data/translated
          echo "‚¨áÔ∏è  Syncing validated translations from ${DEST}validated/translations ..."
          aws s3 sync "${DEST}validated/translations" data/translated --exclude "*" --include "*.json" --endpoint-url "${BACKBLAZE_S3_ENDPOINT}" --only-show-errors || true
          COUNT=$(ls -1 data/translated/*.json 2>/dev/null | wc -l | tr -d ' ')
          echo "Found ${COUNT} validated translation JSON files"
          if [ "${COUNT}" = "0" ]; then
            echo "‚ùå No validated translations found after hydration";
            python -m src.tools.b2_alerts add "hydrate yielded zero translations";
            python -m src.tools.b2_alerts flush || true
            exit 1;
          fi

      - name: Render site (post-hydration)
        if: ${{ inputs.deploy == 'true' }}
        run: |
          set -e
          echo "üé® Rendering site from hydrated data..."
          # PDF generation is now integrated into render.py
          python -m src.render
          echo "üîç Building search index..."
          python -m src.search_index
          # Count generated PDFs
          PDF_COUNT=$(find site/items -name "*.pdf" 2>/dev/null | wc -l | tr -d ' ')
          echo "üìÑ Generated ${PDF_COUNT} English PDFs"
      - name: Persist pipeline outputs to Backblaze B2 (JSON only)
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.BACKBLAZE_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.BACKBLAZE_APPLICATION_KEY }}
          B2_S3_ENDPOINT: ${{ secrets.BACKBLAZE_S3_ENDPOINT }}
          B2_BUCKET: ${{ secrets.BACKBLAZE_BUCKET }}
          B2_PREFIX: ${{ secrets.BACKBLAZE_PREFIX }}
        run: |
          set -e
          if [ -z "${AWS_ACCESS_KEY_ID}" ] || [ -z "${AWS_SECRET_ACCESS_KEY}" ] || [ -z "${B2_S3_ENDPOINT}" ] || [ -z "${B2_BUCKET}" ]; then
            echo "‚ÑπÔ∏è Backblaze B2 secrets not set; skipping persistence"
            exit 0
          fi
          python -m pip install --upgrade pip
          pip install awscli
          export AWS_DEFAULT_REGION=us-west-004
          DEST="s3://${B2_BUCKET}/${B2_PREFIX}"
          echo "üì§ Persisting JSON outputs to ${DEST} via ${B2_S3_ENDPOINT}"
          if [ -f "data/records/chinaxiv_${{ inputs.month }}.json" ]; then
            aws s3 cp "data/records/chinaxiv_${{ inputs.month }}.json" "${DEST}records/" --endpoint-url "${B2_S3_ENDPOINT}" --only-show-errors
          fi
          if [ -d data/translated ]; then
            aws s3 cp data/translated "${DEST}translations/" --recursive --exclude "*" --include "*.json" --endpoint-url "${B2_S3_ENDPOINT}" --only-show-errors
          fi
          if [ -f data/selected.json ]; then
            aws s3 cp data/selected.json "${DEST}selections/${GITHUB_RUN_ID}/selected.json" --endpoint-url "${B2_S3_ENDPOINT}" --only-show-errors
          fi
          # CRITICAL: Upload PDFs to B2 for persistence across workflow runs
          echo "üì§ Uploading PDFs to B2 for persistence..."
          if [ -d data/pdfs ] && ls -1 data/pdfs/chinaxiv-${{ inputs.month }}*.pdf 2>/dev/null | head -1 > /dev/null; then
            aws s3 sync data/pdfs "${DEST}pdfs/" \
              --exclude "*" \
              --include "chinaxiv-${{ inputs.month }}*.pdf" \
              --endpoint-url "${B2_S3_ENDPOINT}" \
              --only-show-errors
            PDF_COUNT=$(ls -1 data/pdfs/chinaxiv-${{ inputs.month }}*.pdf 2>/dev/null | wc -l | tr -d ' ')
            echo "‚úÖ Uploaded ${PDF_COUNT} PDFs to B2"
          else
            echo "‚ÑπÔ∏è No PDFs found for month ${{ inputs.month }}"
          fi
      - name: Upload harvest artifacts (records + selection)
        uses: actions/upload-artifact@v4
        with:
          name: harvest-data-${{ inputs.month }}-${{ github.run_id }}
          if-no-files-found: ignore
          retention-days: 90
          path: |
            data/records/chinaxiv_${{ inputs.month }}.json
            data/selected.json
      - name: Persist dedupe state (seen.json)
        run: |
          set -e
          if [ ! -f data/seen.json ]; then
            echo "No seen.json to persist; skipping"
            exit 0
          fi
          echo "üîí Persisting data/seen.json..."
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git add data/seen.json || true
          if git diff --cached --quiet; then
            echo "No changes in seen.json"
          else
            git commit -m "chore(dedupe): update seen.json [skip ci]" || true
            git push || echo "‚ö†Ô∏è Push failed (non-fatal)"
          fi
      - name: Deploy to Cloudflare Pages (optional)
        if: ${{ inputs.deploy == 'true' }}
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CF_API_TOKEN }}
        run: |
          set -e
          echo "üöÄ Deploying to Cloudflare Pages..."
          npm install -g wrangler
          wrangler pages deploy site --project-name chinarxiv
      - name: Flush B2 alert buffer (if any)
        run: |
          python -m src.tools.b2_alerts flush || true
          echo "‚úÖ Deployment completed"
