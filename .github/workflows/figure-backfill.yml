name: figure-backfill

on:
  workflow_dispatch:
    inputs:
      month:
        description: 'Month to process figures for (YYYYMM)'
        required: true
      workers:
        description: 'Parallel paper workers (default: 8)'
        required: false
        default: '8'
      figure_concurrent:
        description: 'Concurrent figures per paper (default: 8)'
        required: false
        default: '8'
      limit:
        description: 'Max papers to process (0 = all)'
        required: false
        default: '0'
      cs_ai_only:
        description: 'Filter to CS/AI papers only (true/false)'
        required: false
        default: 'true'

  # Allow calling from figure-gate.yml via orchestrator
  workflow_call:
    inputs:
      month:
        description: 'Month to process figures for (YYYYMM)'
        type: string
        required: true
      workers:
        description: 'Parallel paper workers'
        type: string
        required: false
        default: '8'
      figure_concurrent:
        description: 'Concurrent figures per paper'
        type: string
        required: false
        default: '8'
      limit:
        description: 'Max papers to process (0 = all)'
        type: string
        required: false
        default: '0'
      cs_ai_only:
        description: 'Filter to CS/AI papers only'
        type: string
        required: false
        default: 'true'
    secrets:
      GEMINI_API_KEY:
        required: true
      MOONDREAM_API_KEY:
        required: true
      BACKBLAZE_KEY_ID:
        required: true
      BACKBLAZE_APPLICATION_KEY:
        required: true
      BACKBLAZE_S3_ENDPOINT:
        required: true
      BACKBLAZE_BUCKET:
        required: true
      BACKBLAZE_PREFIX:
        required: false
      # NOTE: BrightData secrets removed - figure-backfill requires PDFs to already exist in B2
      # Run pdf-backfill.yml first to download missing PDFs

jobs:
  figure-backfill:
    runs-on: ubuntu-latest
    permissions:
      contents: read
    steps:
      - name: Capture run metadata
        id: meta
        run: echo "started_at=$(date -u +%Y-%m-%dT%H:%M:%SZ)" >> $GITHUB_OUTPUT

      - name: Set Backblaze region (job-wide)
        run: echo "AWS_DEFAULT_REGION=us-west-004" >> $GITHUB_ENV

      - uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install awscli

      - name: "Preflight: Validate API keys"
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          MOONDREAM_API_KEY: ${{ secrets.MOONDREAM_API_KEY }}
        run: |
          set -e
          echo "ğŸ” Preflight check: validating API keys"
          if [ -z "${GEMINI_API_KEY}" ]; then
            echo "âŒ GEMINI_API_KEY not set"
            exit 1
          fi
          if [ -z "${MOONDREAM_API_KEY}" ]; then
            echo "âŒ MOONDREAM_API_KEY not set"
            exit 1
          fi
          echo "âœ… API keys present"

      - name: Get validated papers from B2 manifest
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.BACKBLAZE_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.BACKBLAZE_APPLICATION_KEY }}
          BACKBLAZE_S3_ENDPOINT: ${{ secrets.BACKBLAZE_S3_ENDPOINT }}
          BACKBLAZE_BUCKET: ${{ secrets.BACKBLAZE_BUCKET }}
          BACKBLAZE_PREFIX: ${{ secrets.BACKBLAZE_PREFIX }}
        run: |
          set -e
          export AWS_DEFAULT_REGION=us-west-004
          DEST="s3://${BACKBLAZE_BUCKET}/${BACKBLAZE_PREFIX}"

          echo "ğŸ“‹ Listing validated translations for month ${{ inputs.month }}..."
          mkdir -p data/translated

          # Download all validated translations for this month
          aws s3 sync "${DEST}validated/translations" data/translated \
            --exclude "*" \
            --include "chinaxiv-${{ inputs.month }}*.json" \
            --endpoint-url "${BACKBLAZE_S3_ENDPOINT}" \
            --only-show-errors || true

          COUNT=$(ls -1 data/translated/chinaxiv-${{ inputs.month }}*.json 2>/dev/null | wc -l | tr -d ' ')
          echo "Found ${COUNT} validated translations for ${{ inputs.month }}"

          if [ "${COUNT}" = "0" ]; then
            echo "âŒ No validated translations found for ${{ inputs.month }}"
            exit 1
          fi

          # Extract paper IDs
          ls -1 data/translated/chinaxiv-${{ inputs.month }}*.json | \
            xargs -I{} basename {} .json > /tmp/paper_ids.txt

          echo "Paper IDs to process:"
          head -20 /tmp/paper_ids.txt
          echo "PAPER_COUNT=${COUNT}" >> $GITHUB_ENV

      - name: Filter to CS/AI papers (optional)
        if: inputs.cs_ai_only == 'true'
        run: |
          set -e
          echo "ğŸ”¬ Filtering to CS/AI papers only..."

          # Run CS/AI filter on downloaded translations
          python scripts/filter_cs_ai_papers.py \
            --input data/translated \
            --output /tmp/cs_ai_paper_ids.txt \
            --month "${{ inputs.month }}"

          CS_AI_COUNT=$(wc -l < /tmp/cs_ai_paper_ids.txt | tr -d ' ')
          echo "Found ${CS_AI_COUNT} CS/AI papers out of ${{ env.PAPER_COUNT }}"

          if [ "${CS_AI_COUNT}" = "0" ]; then
            echo "â„¹ï¸ No CS/AI papers found for ${{ inputs.month }}, skipping figure processing"
            echo "SKIP_FIGURES=true" >> $GITHUB_ENV
          else
            # Replace paper_ids.txt with filtered list
            cp /tmp/cs_ai_paper_ids.txt /tmp/paper_ids.txt
            echo "CS_AI_COUNT=${CS_AI_COUNT}" >> $GITHUB_ENV
            echo "PAPER_COUNT=${CS_AI_COUNT}" >> $GITHUB_ENV
          fi

      - name: Sync existing PDFs from B2 (skip already downloaded)
        if: env.SKIP_FIGURES != 'true'
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.BACKBLAZE_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.BACKBLAZE_APPLICATION_KEY }}
          BACKBLAZE_S3_ENDPOINT: ${{ secrets.BACKBLAZE_S3_ENDPOINT }}
          BACKBLAZE_BUCKET: ${{ secrets.BACKBLAZE_BUCKET }}
          BACKBLAZE_PREFIX: ${{ secrets.BACKBLAZE_PREFIX }}
        run: |
          set -e
          export AWS_DEFAULT_REGION=us-west-004
          DEST="s3://${BACKBLAZE_BUCKET}/${BACKBLAZE_PREFIX}"
          mkdir -p data/pdfs

          echo "â¬‡ï¸ Syncing existing PDFs from B2 for ${{ inputs.month }}..."
          aws s3 sync "${DEST}pdfs/" data/pdfs \
            --exclude "*" \
            --include "chinaxiv-${{ inputs.month }}*.pdf" \
            --endpoint-url "${BACKBLAZE_S3_ENDPOINT}" \
            --only-show-errors || true

          CACHED=$(ls -1 data/pdfs/chinaxiv-${{ inputs.month }}*.pdf 2>/dev/null | wc -l | tr -d ' ')
          echo "âœ… Found ${CACHED} cached PDFs in B2"

      - name: Verify PDF availability (pre-flight check)
        if: env.SKIP_FIGURES != 'true'
        run: |
          set -e
          PAPER_COUNT=$(wc -l < /tmp/paper_ids.txt 2>/dev/null | tr -d ' ' || echo "0")
          PDF_COUNT=$(ls -1 data/pdfs/chinaxiv-${{ inputs.month }}*.pdf 2>/dev/null | wc -l | tr -d ' ')

          echo ""
          echo "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—"
          echo "â•‘               PDF AVAILABILITY CHECK                        â•‘"
          echo "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£"
          echo "â•‘ Papers to process: ${PAPER_COUNT}"
          echo "â•‘ PDFs in B2:        ${PDF_COUNT}"

          # Count how many papers have matching PDFs
          MATCHED=0
          MISSING_LIST=""
          while read -r paper_id; do
            if [ -f "data/pdfs/${paper_id}.pdf" ]; then
              MATCHED=$((MATCHED + 1))
            else
              if [ -z "$MISSING_LIST" ]; then
                MISSING_LIST="$paper_id"
              else
                MISSING_LIST="$MISSING_LIST, $paper_id"
              fi
            fi
          done < /tmp/paper_ids.txt

          MISSING=$((PAPER_COUNT - MATCHED))
          echo "â•‘ Papers with PDFs:  ${MATCHED}"
          echo "â•‘ Papers missing:    ${MISSING}"
          echo "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo ""

          if [ "${MATCHED}" -eq 0 ]; then
            echo "âŒ FATAL: No papers have PDFs in B2"
            echo ""
            echo "ğŸ“‹ To fix this, run pdf-backfill first:"
            echo "   gh workflow run pdf-backfill.yml -f month=${{ inputs.month }}"
            echo ""
            exit 1
          fi

          if [ "${MISSING}" -gt 0 ]; then
            echo "âš ï¸  ${MISSING} papers will be SKIPPED (no PDF in B2)"
            echo "   Missing: ${MISSING_LIST:0:200}..."
            echo ""
            echo "ğŸ“‹ To process these papers, run pdf-backfill first:"
            echo "   gh workflow run pdf-backfill.yml -f month=${{ inputs.month }}"
            echo ""
          fi

          echo "âœ… Will process ${MATCHED} papers with available PDFs"
          echo "PAPERS_WITH_PDFS=${MATCHED}" >> $GITHUB_ENV

      - name: Run figure pipeline
        if: env.SKIP_FIGURES != 'true'
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          MOONDREAM_API_KEY: ${{ secrets.MOONDREAM_API_KEY }}
          FIGURE_CONCURRENT: ${{ inputs.figure_concurrent }}
          # B2 credentials for figure upload and status writing
          AWS_ACCESS_KEY_ID: ${{ secrets.BACKBLAZE_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.BACKBLAZE_APPLICATION_KEY }}
          BACKBLAZE_KEY_ID: ${{ secrets.BACKBLAZE_KEY_ID }}
          BACKBLAZE_APPLICATION_KEY: ${{ secrets.BACKBLAZE_APPLICATION_KEY }}
          BACKBLAZE_S3_ENDPOINT: ${{ secrets.BACKBLAZE_S3_ENDPOINT }}
          BACKBLAZE_BUCKET: ${{ secrets.BACKBLAZE_BUCKET }}
          BACKBLAZE_PREFIX: ${{ secrets.BACKBLAZE_PREFIX }}
          # GitHub run info for status writer
          GITHUB_RUN_ID: ${{ github.run_id }}
          GITHUB_REPOSITORY: ${{ github.repository }}
        run: |
          set -e

          echo "ğŸ¨ Processing figures for month ${{ inputs.month }}..."
          echo "Paper workers: ${{ inputs.workers }}"
          echo "Figure concurrent: ${{ inputs.figure_concurrent }}"
          echo "Limit: ${{ inputs.limit }}"

          # Run the figure pipeline with status writing
          python -c "
          import os
          import sys
          from pathlib import Path

          sys.path.insert(0, str(Path.cwd()))

          from src.figure_pipeline import FigurePipeline, PipelineConfig
          from src import status_writer

          # Get paper IDs from filtered file (or all translations if not filtered)
          limit = int('${{ inputs.limit }}') or None
          workers = int('${{ inputs.workers }}')
          month = '${{ inputs.month }}'

          paper_ids = []

          # Read paper IDs from filter output
          paper_ids_file = Path('/tmp/paper_ids.txt')
          if paper_ids_file.exists():
              with open(paper_ids_file) as f:
                  for line in f:
                      paper_id = line.strip()
                      if paper_id:
                          pdf_path = Path('data/pdfs') / f'{paper_id}.pdf'
                          if pdf_path.exists():
                              paper_ids.append(paper_id)

          if limit:
              paper_ids = paper_ids[:limit]

          print(f'Processing {len(paper_ids)} papers with {workers} workers')

          if not paper_ids:
              print('No papers to process')
              sys.exit(0)

          # Start status tracking
          status_writer.start_stage('figures', total=len(paper_ids), month=month)

          config = PipelineConfig(
              gemini_api_key=os.environ.get('GEMINI_API_KEY'),
              moondream_api_key=os.environ.get('MOONDREAM_API_KEY'),
              pdf_dir='data/pdfs',
              dry_run=False,
          )

          # Process papers in parallel with real-time status updates
          from concurrent.futures import ThreadPoolExecutor, as_completed
          from src.figure_pipeline import FigureProcessingResult

          def process_single(paper_id):
              '''Process single paper, return (paper_id, result, error).'''
              try:
                  p = FigurePipeline(config)  # Fresh instance per thread
                  result = p.process_paper(paper_id)
                  return (paper_id, result, None)
              except Exception as e:
                  return (paper_id, None, str(e))

          results = []
          success = True
          completed_count = 0
          try:
              with ThreadPoolExecutor(max_workers=workers) as executor:
                  futures = {executor.submit(process_single, pid): pid for pid in paper_ids}

                  for future in as_completed(futures):
                      paper_id = futures[future]
                      completed_count += 1
                      pid, result, error = future.result()

                      if error:
                          print(f'[{completed_count}/{len(paper_ids)}] {paper_id}: ERROR - {error}')
                          results.append(FigureProcessingResult(paper_id=paper_id, failed=1))
                          status_writer.record_completion(success=False)
                      else:
                          paper_success = result.translated > 0 or result.total_figures == 0
                          print(f'[{completed_count}/{len(paper_ids)}] {paper_id}: {result.translated}/{result.total_figures} figures')
                          results.append(result)
                          status_writer.record_completion(success=paper_success)

              # Summary
              total_figures = sum(r.total_figures for r in results)
              translated = sum(r.translated for r in results)
              failed = sum(r.failed for r in results)

              print(f'\\n=== SUMMARY ===')
              print(f'Papers processed: {len(results)}')
              print(f'Total figures: {total_figures}')
              print(f'Translated: {translated}')
              print(f'Failed: {failed}')

              if failed > total_figures * 0.5 and total_figures > 0:
                  print('âš ï¸ High failure rate')
                  success = False

              # Pass actual figures count for inventory (not paper count)
              status_writer.finish_stage(success=success, figures_translated=translated)

              if not success:
                  sys.exit(1)

          except Exception as e:
              print(f'Pipeline failed: {e}')
              status_writer.write_failure(str(e))
              sys.exit(1)
          "

      - name: Upload figures to B2
        if: env.SKIP_FIGURES != 'true'
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.BACKBLAZE_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.BACKBLAZE_APPLICATION_KEY }}
          BACKBLAZE_S3_ENDPOINT: ${{ secrets.BACKBLAZE_S3_ENDPOINT }}
          BACKBLAZE_BUCKET: ${{ secrets.BACKBLAZE_BUCKET }}
          BACKBLAZE_PREFIX: ${{ secrets.BACKBLAZE_PREFIX }}
        run: |
          set -e
          export AWS_DEFAULT_REGION=us-west-004
          DEST="s3://${BACKBLAZE_BUCKET}/${BACKBLAZE_PREFIX}"

          echo "ğŸ“¤ Uploading figures to B2..."

          # Upload any generated figures
          if [ -d "data/figures" ]; then
            aws s3 sync data/figures "${DEST}figures/" \
              --endpoint-url "${BACKBLAZE_S3_ENDPOINT}" \
              --only-show-errors
            echo "âœ… Figures uploaded"
          else
            echo "â„¹ï¸ No figures directory found"
          fi

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: figure-results-${{ inputs.month }}-${{ github.run_id }}
          if-no-files-found: ignore
          retention-days: 30
          path: |
            data/figures/
            reports/figure_*.json
