name: figure-backfill

on:
  workflow_dispatch:
    inputs:
      month:
        description: 'Month to process figures for (YYYYMM)'
        required: true
      workers:
        description: 'Parallel paper workers (default: 8)'
        required: false
        default: '8'
      figure_concurrent:
        description: 'Concurrent figures per paper (default: 8)'
        required: false
        default: '8'
      limit:
        description: 'Max papers to process (0 = all)'
        required: false
        default: '0'
      cs_ai_only:
        description: 'Filter to CS/AI papers only (true/false)'
        required: false
        default: 'true'

jobs:
  figure-backfill:
    runs-on: ubuntu-latest
    permissions:
      contents: read
    steps:
      - name: Capture run metadata
        id: meta
        run: echo "started_at=$(date -u +%Y-%m-%dT%H:%M:%SZ)" >> $GITHUB_OUTPUT

      - name: Set Backblaze region (job-wide)
        run: echo "AWS_DEFAULT_REGION=us-west-004" >> $GITHUB_ENV

      - uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install awscli

      - name: "Preflight: Validate API keys"
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          MOONDREAM_API_KEY: ${{ secrets.MOONDREAM_API_KEY }}
        run: |
          set -e
          echo "üîé Preflight check: validating API keys"
          if [ -z "${GEMINI_API_KEY}" ]; then
            echo "‚ùå GEMINI_API_KEY not set"
            exit 1
          fi
          if [ -z "${MOONDREAM_API_KEY}" ]; then
            echo "‚ùå MOONDREAM_API_KEY not set"
            exit 1
          fi
          echo "‚úÖ API keys present"

      - name: Get validated papers from B2 manifest
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.BACKBLAZE_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.BACKBLAZE_APPLICATION_KEY }}
          BACKBLAZE_S3_ENDPOINT: ${{ secrets.BACKBLAZE_S3_ENDPOINT }}
          BACKBLAZE_BUCKET: ${{ secrets.BACKBLAZE_BUCKET }}
          BACKBLAZE_PREFIX: ${{ secrets.BACKBLAZE_PREFIX }}
        run: |
          set -e
          export AWS_DEFAULT_REGION=us-west-004
          DEST="s3://${BACKBLAZE_BUCKET}/${BACKBLAZE_PREFIX}"

          echo "üìã Listing validated translations for month ${{ inputs.month }}..."
          mkdir -p data/translated

          # Download all validated translations for this month
          aws s3 sync "${DEST}validated/translations" data/translated \
            --exclude "*" \
            --include "chinaxiv-${{ inputs.month }}*.json" \
            --endpoint-url "${BACKBLAZE_S3_ENDPOINT}" \
            --only-show-errors || true

          COUNT=$(ls -1 data/translated/chinaxiv-${{ inputs.month }}*.json 2>/dev/null | wc -l | tr -d ' ')
          echo "Found ${COUNT} validated translations for ${{ inputs.month }}"

          if [ "${COUNT}" = "0" ]; then
            echo "‚ùå No validated translations found for ${{ inputs.month }}"
            exit 1
          fi

          # Extract paper IDs
          ls -1 data/translated/chinaxiv-${{ inputs.month }}*.json | \
            xargs -I{} basename {} .json > /tmp/paper_ids.txt

          echo "Paper IDs to process:"
          head -20 /tmp/paper_ids.txt
          echo "PAPER_COUNT=${COUNT}" >> $GITHUB_ENV

      - name: Filter to CS/AI papers (optional)
        if: inputs.cs_ai_only == 'true'
        run: |
          set -e
          echo "üî¨ Filtering to CS/AI papers only..."

          # Run CS/AI filter on downloaded translations
          python scripts/filter_cs_ai_papers.py \
            --input data/translated \
            --output /tmp/cs_ai_paper_ids.txt \
            --month "${{ inputs.month }}"

          CS_AI_COUNT=$(wc -l < /tmp/cs_ai_paper_ids.txt | tr -d ' ')
          echo "Found ${CS_AI_COUNT} CS/AI papers out of ${{ env.PAPER_COUNT }}"

          if [ "${CS_AI_COUNT}" = "0" ]; then
            echo "‚ÑπÔ∏è No CS/AI papers found for ${{ inputs.month }}, skipping figure processing"
            echo "SKIP_FIGURES=true" >> $GITHUB_ENV
          else
            # Replace paper_ids.txt with filtered list
            cp /tmp/cs_ai_paper_ids.txt /tmp/paper_ids.txt
            echo "CS_AI_COUNT=${CS_AI_COUNT}" >> $GITHUB_ENV
            echo "PAPER_COUNT=${CS_AI_COUNT}" >> $GITHUB_ENV
          fi

      - name: Sync existing PDFs from B2 (skip already downloaded)
        if: env.SKIP_FIGURES != 'true'
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.BACKBLAZE_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.BACKBLAZE_APPLICATION_KEY }}
          BACKBLAZE_S3_ENDPOINT: ${{ secrets.BACKBLAZE_S3_ENDPOINT }}
          BACKBLAZE_BUCKET: ${{ secrets.BACKBLAZE_BUCKET }}
          BACKBLAZE_PREFIX: ${{ secrets.BACKBLAZE_PREFIX }}
        run: |
          set -e
          export AWS_DEFAULT_REGION=us-west-004
          DEST="s3://${BACKBLAZE_BUCKET}/${BACKBLAZE_PREFIX}"
          mkdir -p data/pdfs

          echo "‚¨áÔ∏è Syncing existing PDFs from B2 for ${{ inputs.month }}..."
          aws s3 sync "${DEST}pdfs/" data/pdfs \
            --exclude "*" \
            --include "chinaxiv-${{ inputs.month }}*.pdf" \
            --endpoint-url "${BACKBLAZE_S3_ENDPOINT}" \
            --only-show-errors || true

          CACHED=$(ls -1 data/pdfs/chinaxiv-${{ inputs.month }}*.pdf 2>/dev/null | wc -l | tr -d ' ')
          echo "‚úÖ Found ${CACHED} cached PDFs in B2"

      - name: Download missing PDFs from ChinaXiv
        if: env.SKIP_FIGURES != 'true'
        env:
          BRIGHTDATA_API_KEY: ${{ secrets.BRIGHTDATA_API_KEY }}
          BRIGHTDATA_ZONE: ${{ secrets.BRIGHTDATA_ZONE }}
          BRIGHTDATA_UNLOCKER_ZONE: ${{ secrets.BRIGHTDATA_UNLOCKER_ZONE }}
          BRIGHTDATA_UNLOCKER_PASSWORD: ${{ secrets.BRIGHTDATA_UNLOCKER_PASSWORD }}
        run: |
          set -e

          echo "üì• Downloading missing PDFs for ${{ env.PAPER_COUNT }} papers..."

          # Ensure we have records for this month
          REC="data/records/chinaxiv_${{ inputs.month }}.json"
          if [ ! -f "$REC" ]; then
            echo "üì• Harvesting records for ${{ inputs.month }}..."
            python -m src.harvest_chinaxiv_optimized --month "${{ inputs.month }}" || {
              echo "‚ö†Ô∏è Harvest failed, trying to continue with existing data"
            }
          fi

          if [ -f "$REC" ]; then
            # This skips PDFs that already exist locally (from B2 sync)
            python scripts/download_missing_pdfs.py --months "${{ inputs.month }}" || {
              echo "‚ö†Ô∏è Some PDF downloads failed"
            }
          fi

          PDF_COUNT=$(ls -1 data/pdfs/chinaxiv-${{ inputs.month }}*.pdf 2>/dev/null | wc -l | tr -d ' ')
          echo "Total PDFs available: ${PDF_COUNT}"
          echo "PDF_COUNT=${PDF_COUNT}" >> $GITHUB_ENV

      - name: Verify PDF availability
        if: env.SKIP_FIGURES != 'true'
        run: |
          set -e
          PAPER_COUNT=$(wc -l < /tmp/paper_ids.txt 2>/dev/null | tr -d ' ' || echo "0")
          PDF_COUNT=${PDF_COUNT:-0}

          echo "Papers to process: ${PAPER_COUNT}"
          echo "PDFs available: ${PDF_COUNT}"

          if [ "${PDF_COUNT}" -eq 0 ]; then
            echo "‚ùå FATAL: No PDFs available for figure translation"
            echo "Run 'gh workflow run pdf-backfill.yml -f month=${{ inputs.month }}' first"
            exit 1
          fi

          # Count how many papers have matching PDFs
          MATCHED=0
          while read -r paper_id; do
            if [ -f "data/pdfs/${paper_id}.pdf" ]; then
              MATCHED=$((MATCHED + 1))
            fi
          done < /tmp/paper_ids.txt

          echo "Papers with PDFs: ${MATCHED}/${PAPER_COUNT}"

          if [ "${MATCHED}" -eq 0 ]; then
            echo "‚ùå FATAL: No papers have matching PDFs"
            echo "Run 'gh workflow run pdf-backfill.yml -f month=${{ inputs.month }}' first"
            exit 1
          fi

          echo "PAPERS_WITH_PDFS=${MATCHED}" >> $GITHUB_ENV

      - name: Upload PDFs to B2 (for future runs)
        if: env.SKIP_FIGURES != 'true'
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.BACKBLAZE_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.BACKBLAZE_APPLICATION_KEY }}
          BACKBLAZE_S3_ENDPOINT: ${{ secrets.BACKBLAZE_S3_ENDPOINT }}
          BACKBLAZE_BUCKET: ${{ secrets.BACKBLAZE_BUCKET }}
          BACKBLAZE_PREFIX: ${{ secrets.BACKBLAZE_PREFIX }}
        run: |
          set -e
          export AWS_DEFAULT_REGION=us-west-004
          DEST="s3://${BACKBLAZE_BUCKET}/${BACKBLAZE_PREFIX}"

          echo "üì§ Uploading PDFs to B2 for future runs..."
          aws s3 sync data/pdfs "${DEST}pdfs/" \
            --exclude "*" \
            --include "chinaxiv-${{ inputs.month }}*.pdf" \
            --endpoint-url "${BACKBLAZE_S3_ENDPOINT}" \
            --only-show-errors || true
          echo "‚úÖ PDFs synced to B2"

      - name: Run figure pipeline
        if: env.SKIP_FIGURES != 'true'
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          MOONDREAM_API_KEY: ${{ secrets.MOONDREAM_API_KEY }}
          FIGURE_CONCURRENT: ${{ inputs.figure_concurrent }}
          # B2 credentials for figure upload
          BACKBLAZE_KEY_ID: ${{ secrets.BACKBLAZE_KEY_ID }}
          BACKBLAZE_APPLICATION_KEY: ${{ secrets.BACKBLAZE_APPLICATION_KEY }}
          BACKBLAZE_S3_ENDPOINT: ${{ secrets.BACKBLAZE_S3_ENDPOINT }}
          BACKBLAZE_BUCKET: ${{ secrets.BACKBLAZE_BUCKET }}
        run: |
          set -e

          echo "üé® Processing figures for month ${{ inputs.month }}..."
          echo "Paper workers: ${{ inputs.workers }}"
          echo "Figure concurrent: ${{ inputs.figure_concurrent }}"
          echo "Limit: ${{ inputs.limit }}"

          # Run the figure pipeline
          python -c "
          import os
          import sys
          from pathlib import Path

          sys.path.insert(0, str(Path.cwd()))

          from src.figure_pipeline import FigurePipeline, PipelineConfig

          # Get paper IDs from filtered file (or all translations if not filtered)
          limit = int('${{ inputs.limit }}') or None
          workers = int('${{ inputs.workers }}')

          paper_ids = []

          # Read paper IDs from filter output
          paper_ids_file = Path('/tmp/paper_ids.txt')
          if paper_ids_file.exists():
              with open(paper_ids_file) as f:
                  for line in f:
                      paper_id = line.strip()
                      if paper_id:
                          pdf_path = Path('data/pdfs') / f'{paper_id}.pdf'
                          if pdf_path.exists():
                              paper_ids.append(paper_id)

          if limit:
              paper_ids = paper_ids[:limit]

          print(f'Processing {len(paper_ids)} papers with {workers} workers')

          if not paper_ids:
              print('No papers to process')
              sys.exit(0)

          config = PipelineConfig(
              gemini_api_key=os.environ.get('GEMINI_API_KEY'),
              moondream_api_key=os.environ.get('MOONDREAM_API_KEY'),
              pdf_dir='data/pdfs',
              dry_run=False,
          )

          pipeline = FigurePipeline(config)
          results = pipeline.process_batch(paper_ids, workers=workers)

          # Summary
          total_figures = sum(r.total_figures for r in results)
          translated = sum(r.translated for r in results)
          failed = sum(r.failed for r in results)

          print(f'\\n=== SUMMARY ===')
          print(f'Papers processed: {len(results)}')
          print(f'Total figures: {total_figures}')
          print(f'Translated: {translated}')
          print(f'Failed: {failed}')

          if failed > total_figures * 0.5:
              print('‚ö†Ô∏è High failure rate')
              sys.exit(1)
          "

      - name: Upload figures to B2
        if: env.SKIP_FIGURES != 'true'
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.BACKBLAZE_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.BACKBLAZE_APPLICATION_KEY }}
          BACKBLAZE_S3_ENDPOINT: ${{ secrets.BACKBLAZE_S3_ENDPOINT }}
          BACKBLAZE_BUCKET: ${{ secrets.BACKBLAZE_BUCKET }}
          BACKBLAZE_PREFIX: ${{ secrets.BACKBLAZE_PREFIX }}
        run: |
          set -e
          export AWS_DEFAULT_REGION=us-west-004
          DEST="s3://${BACKBLAZE_BUCKET}/${BACKBLAZE_PREFIX}"

          echo "üì§ Uploading figures to B2..."

          # Upload any generated figures
          if [ -d "data/figures" ]; then
            aws s3 sync data/figures "${DEST}figures/" \
              --endpoint-url "${BACKBLAZE_S3_ENDPOINT}" \
              --only-show-errors
            echo "‚úÖ Figures uploaded"
          else
            echo "‚ÑπÔ∏è No figures directory found"
          fi

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: figure-results-${{ inputs.month }}-${{ github.run_id }}
          if-no-files-found: ignore
          retention-days: 30
          path: |
            data/figures/
            reports/figure_*.json
