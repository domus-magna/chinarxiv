name: figure-backfill

on:
  workflow_dispatch:
    inputs:
      month:
        description: 'Month to process figures for (YYYYMM)'
        required: true
      workers:
        description: 'Parallel figure processing workers'
        required: false
        default: '4'
      limit:
        description: 'Max papers to process (0 = all)'
        required: false
        default: '0'

jobs:
  figure-backfill:
    runs-on: ubuntu-latest
    permissions:
      contents: read
    steps:
      - name: Capture run metadata
        id: meta
        run: echo "started_at=$(date -u +%Y-%m-%dT%H:%M:%SZ)" >> $GITHUB_OUTPUT

      - name: Set Backblaze region (job-wide)
        run: echo "AWS_DEFAULT_REGION=us-west-004" >> $GITHUB_ENV

      - uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install awscli

      - name: "Preflight: Validate API keys"
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          MOONDREAM_API_KEY: ${{ secrets.MOONDREAM_API_KEY }}
        run: |
          set -e
          echo "üîé Preflight check: validating API keys"
          if [ -z "${GEMINI_API_KEY}" ]; then
            echo "‚ùå GEMINI_API_KEY not set"
            exit 1
          fi
          if [ -z "${MOONDREAM_API_KEY}" ]; then
            echo "‚ùå MOONDREAM_API_KEY not set"
            exit 1
          fi
          echo "‚úÖ API keys present"

      - name: Get validated papers from B2 manifest
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.BACKBLAZE_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.BACKBLAZE_APPLICATION_KEY }}
          BACKBLAZE_S3_ENDPOINT: ${{ secrets.BACKBLAZE_S3_ENDPOINT }}
          BACKBLAZE_BUCKET: ${{ secrets.BACKBLAZE_BUCKET }}
          BACKBLAZE_PREFIX: ${{ secrets.BACKBLAZE_PREFIX }}
        run: |
          set -e
          export AWS_DEFAULT_REGION=us-west-004
          DEST="s3://${BACKBLAZE_BUCKET}/${BACKBLAZE_PREFIX}"

          echo "üìã Listing validated translations for month ${{ inputs.month }}..."
          mkdir -p data/translated

          # Download all validated translations for this month
          aws s3 sync "${DEST}validated/translations" data/translated \
            --exclude "*" \
            --include "chinaxiv-${{ inputs.month }}*.json" \
            --endpoint-url "${BACKBLAZE_S3_ENDPOINT}" \
            --only-show-errors || true

          COUNT=$(ls -1 data/translated/chinaxiv-${{ inputs.month }}*.json 2>/dev/null | wc -l | tr -d ' ')
          echo "Found ${COUNT} validated translations for ${{ inputs.month }}"

          if [ "${COUNT}" = "0" ]; then
            echo "‚ùå No validated translations found for ${{ inputs.month }}"
            exit 1
          fi

          # Extract paper IDs
          ls -1 data/translated/chinaxiv-${{ inputs.month }}*.json | \
            xargs -I{} basename {} .json > /tmp/paper_ids.txt

          echo "Paper IDs to process:"
          head -20 /tmp/paper_ids.txt
          echo "PAPER_COUNT=${COUNT}" >> $GITHUB_ENV

      - name: Download PDFs from ChinaXiv
        env:
          BRIGHTDATA_API_KEY: ${{ secrets.BRIGHTDATA_API_KEY }}
          BRIGHTDATA_ZONE: ${{ secrets.BRIGHTDATA_ZONE }}
          BRIGHTDATA_UNLOCKER_ZONE: ${{ secrets.BRIGHTDATA_UNLOCKER_ZONE }}
          BRIGHTDATA_UNLOCKER_PASSWORD: ${{ secrets.BRIGHTDATA_UNLOCKER_PASSWORD }}
        run: |
          set -e
          mkdir -p data/pdfs

          echo "üì• Downloading PDFs for ${{ env.PAPER_COUNT }} papers..."

          # Download PDFs using the download script
          # First, ensure we have records for this month
          REC="data/records/chinaxiv_${{ inputs.month }}.json"
          if [ ! -f "$REC" ]; then
            echo "üì• Harvesting records for ${{ inputs.month }}..."
            python -m src.harvest_chinaxiv_optimized --month "${{ inputs.month }}" || {
              echo "‚ö†Ô∏è Harvest failed, trying to continue with existing data"
            }
          fi

          if [ -f "$REC" ]; then
            python scripts/download_missing_pdfs.py --months "${{ inputs.month }}" || {
              echo "‚ö†Ô∏è Some PDF downloads failed"
            }
          fi

          PDF_COUNT=$(ls -1 data/pdfs/chinaxiv-${{ inputs.month }}*.pdf 2>/dev/null | wc -l | tr -d ' ')
          echo "Downloaded ${PDF_COUNT} PDFs"
          echo "PDF_COUNT=${PDF_COUNT}" >> $GITHUB_ENV

      - name: Run figure pipeline
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          MOONDREAM_API_KEY: ${{ secrets.MOONDREAM_API_KEY }}
        run: |
          set -e

          echo "üé® Processing figures for month ${{ inputs.month }}..."
          echo "Workers: ${{ inputs.workers }}"
          echo "Limit: ${{ inputs.limit }}"

          # Run the figure pipeline
          python -c "
          import os
          import sys
          from pathlib import Path

          sys.path.insert(0, str(Path.cwd()))

          from src.figure_pipeline import FigurePipeline, PipelineConfig

          # Get paper IDs from translated files
          translated_dir = Path('data/translated')
          month = '${{ inputs.month }}'
          limit = int('${{ inputs.limit }}') or None
          workers = int('${{ inputs.workers }}')

          paper_ids = []
          for f in sorted(translated_dir.glob(f'chinaxiv-{month}*.json')):
              paper_id = f.stem
              pdf_path = Path('data/pdfs') / f'{paper_id}.pdf'
              if pdf_path.exists():
                  paper_ids.append(paper_id)

          if limit:
              paper_ids = paper_ids[:limit]

          print(f'Processing {len(paper_ids)} papers with {workers} workers')

          if not paper_ids:
              print('No papers to process')
              sys.exit(0)

          config = PipelineConfig(
              gemini_api_key=os.environ.get('GEMINI_API_KEY'),
              moondream_api_key=os.environ.get('MOONDREAM_API_KEY'),
              pdf_dir='data/pdfs',
              dry_run=False,
          )

          pipeline = FigurePipeline(config)
          results = pipeline.process_batch(paper_ids, workers=workers)

          # Summary
          total_figures = sum(r.total_figures for r in results)
          translated = sum(r.translated for r in results)
          failed = sum(r.failed for r in results)

          print(f'\\n=== SUMMARY ===')
          print(f'Papers processed: {len(results)}')
          print(f'Total figures: {total_figures}')
          print(f'Translated: {translated}')
          print(f'Failed: {failed}')

          if failed > total_figures * 0.5:
              print('‚ö†Ô∏è High failure rate')
              sys.exit(1)
          "

      - name: Upload figures to B2
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.BACKBLAZE_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.BACKBLAZE_APPLICATION_KEY }}
          BACKBLAZE_S3_ENDPOINT: ${{ secrets.BACKBLAZE_S3_ENDPOINT }}
          BACKBLAZE_BUCKET: ${{ secrets.BACKBLAZE_BUCKET }}
          BACKBLAZE_PREFIX: ${{ secrets.BACKBLAZE_PREFIX }}
        run: |
          set -e
          export AWS_DEFAULT_REGION=us-west-004
          DEST="s3://${BACKBLAZE_BUCKET}/${BACKBLAZE_PREFIX}"

          echo "üì§ Uploading figures to B2..."

          # Upload any generated figures
          if [ -d "data/figures" ]; then
            aws s3 sync data/figures "${DEST}figures/" \
              --endpoint-url "${BACKBLAZE_S3_ENDPOINT}" \
              --only-show-errors
            echo "‚úÖ Figures uploaded"
          else
            echo "‚ÑπÔ∏è No figures directory found"
          fi

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: figure-results-${{ inputs.month }}-${{ github.run_id }}
          if-no-files-found: ignore
          retention-days: 30
          path: |
            data/figures/
            reports/figure_*.json
