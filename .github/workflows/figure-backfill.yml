name: figure-backfill

on:
  workflow_dispatch:
    inputs:
      month:
        description: 'Month to process figures for (YYYYMM)'
        required: true
      workers:
        description: 'Parallel paper workers (default: 8)'
        required: false
        default: '8'
      figure_concurrent:
        description: 'Concurrent figures per paper (default: 8)'
        required: false
        default: '8'
      limit:
        description: 'Max papers to process (0 = all)'
        required: false
        default: '0'
      cs_ai_only:
        description: 'Filter to CS/AI papers only (true/false)'
        required: false
        default: 'true'

  # Allow calling from figure-gate.yml via orchestrator
  workflow_call:
    inputs:
      month:
        description: 'Month to process figures for (YYYYMM)'
        type: string
        required: true
      workers:
        description: 'Parallel paper workers'
        type: string
        required: false
        default: '8'
      figure_concurrent:
        description: 'Concurrent figures per paper'
        type: string
        required: false
        default: '8'
      limit:
        description: 'Max papers to process (0 = all)'
        type: string
        required: false
        default: '0'
      cs_ai_only:
        description: 'Filter to CS/AI papers only'
        type: string
        required: false
        default: 'true'
    secrets:
      GEMINI_API_KEY:
        required: true
      MOONDREAM_API_KEY:
        required: true
      BACKBLAZE_KEY_ID:
        required: true
      BACKBLAZE_APPLICATION_KEY:
        required: true
      BACKBLAZE_S3_ENDPOINT:
        required: true
      BACKBLAZE_BUCKET:
        required: true
      BACKBLAZE_PREFIX:
        required: false
      # BrightData secrets for automatic PDF acquisition (optional - only needed if PDFs missing)
      BRIGHTDATA_API_KEY:
        required: false
      BRIGHTDATA_ZONE:
        required: false
      BRIGHTDATA_UNLOCKER_ZONE:
        required: false
      BRIGHTDATA_UNLOCKER_PASSWORD:
        required: false

jobs:
  figure-backfill:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      actions: write  # For triggering backfill.yml if no translations exist
    steps:
      - name: Capture run metadata
        id: meta
        run: echo "started_at=$(date -u +%Y-%m-%dT%H:%M:%SZ)" >> $GITHUB_OUTPUT

      - name: Set Backblaze region (job-wide)
        run: echo "AWS_DEFAULT_REGION=us-west-004" >> $GITHUB_ENV

      - uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: "Pre-check: Validate translations exist or trigger backfill"
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          AWS_ACCESS_KEY_ID: ${{ secrets.BACKBLAZE_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.BACKBLAZE_APPLICATION_KEY }}
          BACKBLAZE_S3_ENDPOINT: ${{ secrets.BACKBLAZE_S3_ENDPOINT }}
          BACKBLAZE_BUCKET: ${{ secrets.BACKBLAZE_BUCKET }}
          BACKBLAZE_PREFIX: ${{ secrets.BACKBLAZE_PREFIX }}
        run: |
          set -euo pipefail
          MONTH="${{ inputs.month }}"
          DEST="s3://${BACKBLAZE_BUCKET}/${BACKBLAZE_PREFIX}"
          RUN_NAME="backfill-${MONTH}"

          echo "üîç Checking for validated translations for ${MONTH}..."

          # Count translations (use pre-installed aws cli, grep for .json files)
          # Note: aws s3 ls returns non-zero when no files match prefix, so use || true
          # and check for actual error messages to distinguish auth failures from empty results
          LIST_OUTPUT=$(aws s3 ls "${DEST}validated/translations/chinaxiv-${MONTH}" \
              --endpoint-url "${BACKBLAZE_S3_ENDPOINT}" 2>&1 || true)

          # Check for actual B2 access errors (auth, endpoint, etc.)
          if echo "$LIST_OUTPUT" | grep -qiE "(AccessDenied|InvalidAccessKeyId|SignatureDoesNotMatch|NoSuchBucket|Unable to locate credentials)"; then
            echo "‚ùå B2 ACCESS FAILED (MANDATORY per CLAUDE.md):"
            echo "$LIST_OUTPUT"
            echo ""
            echo "Check BACKBLAZE_KEY_ID, BACKBLAZE_APPLICATION_KEY, BACKBLAZE_S3_ENDPOINT secrets."
            exit 1
          fi

          # Count .json files - use wc -l instead of grep -c to avoid multi-line COUNT issue
          # (grep -c outputs "0" then || echo "0" adds another, making COUNT="0\n0")
          # Note: grep || true needed because set -o pipefail would fail on no matches
          COUNT=$(echo "$LIST_OUTPUT" | { grep '\.json$' || true; } | wc -l | tr -d ' ')
          echo "Found ${COUNT} validated translations for ${MONTH}"

          if [ "${COUNT}" = "0" ]; then
            echo ""
            echo "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó"
            echo "‚ïë  ‚ö° NO TRANSLATIONS FOR ${MONTH} - CHECKING FOR EXISTING BACKFILL ‚ïë"
            echo "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù"

            # Idempotency: Check if backfill is already queued or running for this month
            # Uses run-name (backfill-YYYYMM) which is set in backfill.yml
            EXISTING=$(gh run list --workflow=backfill.yml \
              --json status,name,url \
              --jq ".[] | select(.name == \"${RUN_NAME}\") | select(.status == \"queued\" or .status == \"in_progress\")" \
              2>/dev/null || echo "")

            if [ -n "$EXISTING" ]; then
              echo "‚è≥ Backfill already queued or in progress for ${MONTH}:"
              gh run list --workflow=backfill.yml --limit 5 \
                --json status,name,url --jq ".[] | select(.name == \"${RUN_NAME}\") | \"\(.status): \(.url)\""
              echo ""
              echo "Wait for it to complete, then re-run figure-backfill."
              exit 1
            fi

            echo "üöÄ Triggering text backfill for ${MONTH}..."
            if ! gh workflow run backfill.yml -f month=${MONTH} 2>&1; then
              echo "‚ùå Failed to trigger backfill. Check GH_TOKEN permissions (needs actions:write)."
              exit 1
            fi

            # Wait for GitHub to register the run, then fetch URL by run-name
            sleep 5
            RUN_URL=$(gh run list --workflow=backfill.yml --limit 5 \
              --json name,url --jq ".[] | select(.name == \"${RUN_NAME}\") | .url" \
              2>/dev/null | head -1 || echo "")

            echo ""
            echo "‚úÖ Text backfill triggered for ${MONTH}"
            if [ -n "$RUN_URL" ]; then
              echo "   üìé $RUN_URL"
            else
              echo "   (Run URL not yet available - check: gh run list --workflow=backfill.yml)"
            fi
            echo ""
            echo "üìã Next steps:"
            echo "   1. Wait for text backfill to complete (~1-2 hours)"
            echo "   2. Re-run: gh workflow run figure-backfill.yml -f month=${MONTH}"
            echo ""
            echo "   Monitor: gh run list --workflow=backfill.yml --limit 3"
            echo ""
            echo "‚ö†Ô∏è  This job will now exit (intentional failure)."
            exit 1
          fi

          echo "‚úÖ Pre-check passed: ${COUNT} translations available for ${MONTH}"

      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install awscli

      - name: "Preflight: Validate API keys"
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          MOONDREAM_API_KEY: ${{ secrets.MOONDREAM_API_KEY }}
        run: |
          set -e
          echo "üîé Preflight check: validating API keys"
          if [ -z "${GEMINI_API_KEY}" ]; then
            echo "‚ùå GEMINI_API_KEY not set"
            exit 1
          fi
          if [ -z "${MOONDREAM_API_KEY}" ]; then
            echo "‚ùå MOONDREAM_API_KEY not set"
            exit 1
          fi
          echo "‚úÖ API keys present"

      - name: Get validated papers from B2 manifest
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.BACKBLAZE_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.BACKBLAZE_APPLICATION_KEY }}
          BACKBLAZE_S3_ENDPOINT: ${{ secrets.BACKBLAZE_S3_ENDPOINT }}
          BACKBLAZE_BUCKET: ${{ secrets.BACKBLAZE_BUCKET }}
          BACKBLAZE_PREFIX: ${{ secrets.BACKBLAZE_PREFIX }}
        run: |
          set -e
          export AWS_DEFAULT_REGION=us-west-004
          DEST="s3://${BACKBLAZE_BUCKET}/${BACKBLAZE_PREFIX}"

          echo "üìã Listing validated translations for month ${{ inputs.month }}..."
          mkdir -p data/translated

          # Download all validated translations for this month
          aws s3 sync "${DEST}validated/translations" data/translated \
            --exclude "*" \
            --include "chinaxiv-${{ inputs.month }}*.json" \
            --endpoint-url "${BACKBLAZE_S3_ENDPOINT}" \
            --only-show-errors || true

          COUNT=$(ls -1 data/translated/chinaxiv-${{ inputs.month }}*.json 2>/dev/null | wc -l | tr -d ' ')
          echo "Found ${COUNT} validated translations for ${{ inputs.month }}"

          if [ "${COUNT}" = "0" ]; then
            echo "‚ùå No validated translations found for ${{ inputs.month }}"
            exit 1
          fi

          # Extract paper IDs
          ls -1 data/translated/chinaxiv-${{ inputs.month }}*.json | \
            xargs -I{} basename {} .json > /tmp/paper_ids.txt

          echo "Paper IDs to process:"
          head -20 /tmp/paper_ids.txt
          echo "PAPER_COUNT=${COUNT}" >> $GITHUB_ENV

      - name: Filter to CS/AI papers (optional)
        if: inputs.cs_ai_only == 'true'
        run: |
          set -e
          echo "üî¨ Filtering to CS/AI papers only..."

          # Run CS/AI filter on downloaded translations
          python scripts/filter_cs_ai_papers.py \
            --input data/translated \
            --output /tmp/cs_ai_paper_ids.txt \
            --month "${{ inputs.month }}"

          CS_AI_COUNT=$(wc -l < /tmp/cs_ai_paper_ids.txt | tr -d ' ')
          echo "Found ${CS_AI_COUNT} CS/AI papers out of ${{ env.PAPER_COUNT }}"

          if [ "${CS_AI_COUNT}" = "0" ]; then
            echo "‚ÑπÔ∏è No CS/AI papers found for ${{ inputs.month }}, skipping figure processing"
            echo "SKIP_FIGURES=true" >> $GITHUB_ENV
          else
            # Replace paper_ids.txt with filtered list
            cp /tmp/cs_ai_paper_ids.txt /tmp/paper_ids.txt
            echo "CS_AI_COUNT=${CS_AI_COUNT}" >> $GITHUB_ENV
            echo "PAPER_COUNT=${CS_AI_COUNT}" >> $GITHUB_ENV
          fi

      - name: Sync existing PDFs from B2 (skip already downloaded)
        if: env.SKIP_FIGURES != 'true'
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.BACKBLAZE_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.BACKBLAZE_APPLICATION_KEY }}
          BACKBLAZE_S3_ENDPOINT: ${{ secrets.BACKBLAZE_S3_ENDPOINT }}
          BACKBLAZE_BUCKET: ${{ secrets.BACKBLAZE_BUCKET }}
          BACKBLAZE_PREFIX: ${{ secrets.BACKBLAZE_PREFIX }}
        run: |
          set -e
          export AWS_DEFAULT_REGION=us-west-004
          DEST="s3://${BACKBLAZE_BUCKET}/${BACKBLAZE_PREFIX}"
          mkdir -p data/pdfs

          echo "‚¨áÔ∏è Syncing existing PDFs from B2 for ${{ inputs.month }}..."
          aws s3 sync "${DEST}pdfs/" data/pdfs \
            --exclude "*" \
            --include "chinaxiv-${{ inputs.month }}*.pdf" \
            --endpoint-url "${BACKBLAZE_S3_ENDPOINT}" \
            --only-show-errors || true

          CACHED=$(ls -1 data/pdfs/chinaxiv-${{ inputs.month }}*.pdf 2>/dev/null | wc -l | tr -d ' ')
          echo "‚úÖ Found ${CACHED} cached PDFs in B2"

      - name: Acquire missing PDFs (automatic)
        if: env.SKIP_FIGURES != 'true'
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.BACKBLAZE_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.BACKBLAZE_APPLICATION_KEY }}
          BACKBLAZE_S3_ENDPOINT: ${{ secrets.BACKBLAZE_S3_ENDPOINT }}
          BACKBLAZE_BUCKET: ${{ secrets.BACKBLAZE_BUCKET }}
          BACKBLAZE_PREFIX: ${{ secrets.BACKBLAZE_PREFIX }}
          BRIGHTDATA_API_KEY: ${{ secrets.BRIGHTDATA_API_KEY }}
          BRIGHTDATA_ZONE: ${{ secrets.BRIGHTDATA_ZONE }}
          BRIGHTDATA_UNLOCKER_ZONE: ${{ secrets.BRIGHTDATA_UNLOCKER_ZONE }}
          BRIGHTDATA_UNLOCKER_PASSWORD: ${{ secrets.BRIGHTDATA_UNLOCKER_PASSWORD }}
          BRIGHTDATA_BROWSER_WSS: ${{ secrets.BRIGHTDATA_BROWSER_WSS }}
        run: |
          set -e
          export AWS_DEFAULT_REGION=us-west-004
          DEST="s3://${BACKBLAZE_BUCKET}/${BACKBLAZE_PREFIX}"
          MONTH="${{ inputs.month }}"

          # NOTE: This runs AFTER B2 sync, so we're comparing against actual B2 contents
          # /tmp/paper_ids.txt was created by "Get validated papers" step and optionally
          # filtered by CS/AI filter step
          # Format: one paper ID per line, e.g., chinaxiv-202411.00001

          echo "::group::Step 1/6: Detecting missing PDFs ($(date +%H:%M:%S))"
          PAPER_COUNT=$(wc -l < /tmp/paper_ids.txt | tr -d ' ')
          > /tmp/missing_paper_ids.txt  # truncate
          while read -r paper_id; do
            [ -z "$paper_id" ] && continue  # skip blank lines
            if [ ! -f "data/pdfs/${paper_id}.pdf" ]; then
              echo "$paper_id" >> /tmp/missing_paper_ids.txt
            fi
          done < /tmp/paper_ids.txt

          # Dedupe (shouldn't be needed but defensive)
          sort -u /tmp/missing_paper_ids.txt -o /tmp/missing_paper_ids.txt
          MISSING_COUNT=$(wc -l < /tmp/missing_paper_ids.txt | tr -d ' ')
          echo "Total papers: ${PAPER_COUNT}, Missing PDFs: ${MISSING_COUNT}"
          echo "::endgroup::"

          if [ "$MISSING_COUNT" -gt 0 ]; then
            echo ""
            echo "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó"
            echo "‚ïë         AUTOMATIC PDF ACQUISITION                          ‚ïë"
            echo "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£"
            echo "‚ïë Selection: ${PAPER_COUNT} papers total                     ‚ïë"
            echo "‚ïë Missing:   ${MISSING_COUNT} papers need PDFs               ‚ïë"
            echo "‚ïë ‚è±Ô∏è  Adds ~5-15 minutes for harvest + download              ‚ïë"
            echo "‚ïë ‚ö†Ô∏è  No retry on failure - investigate if job fails         ‚ïë"
            echo "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù"
            echo ""

            # Log sample of missing IDs for traceability (first 5)
            echo "Missing paper IDs (sample):"
            head -5 /tmp/missing_paper_ids.txt | sed 's/^/  /'
            echo ""

            echo "::group::Step 2/6: Validating BrightData credentials ($(date +%H:%M:%S))"
            MISSING_CREDS=""
            [ -z "${BRIGHTDATA_API_KEY}" ] && MISSING_CREDS="BRIGHTDATA_API_KEY"
            [ -z "${BRIGHTDATA_ZONE}" ] && MISSING_CREDS="${MISSING_CREDS:+$MISSING_CREDS, }BRIGHTDATA_ZONE"
            if [ -n "${MISSING_CREDS}" ]; then
              echo "‚ùå FATAL: Missing secrets: ${MISSING_CREDS}"
              echo "   Run pdf-backfill.yml first: gh workflow run pdf-backfill.yml -f month=$MONTH"
              echo "::endgroup::"
              exit 1
            fi
            echo "‚úì BrightData credentials present"
            echo "::endgroup::"

            echo "::group::Step 3/6: Harvesting fresh metadata ($(date +%H:%M:%S))"
            RECORDS_PATH="data/records/chinaxiv_${MONTH}.json"
            echo "üì• Harvesting fresh metadata for $MONTH..."
            echo "   Records will be written to: ${RECORDS_PATH}"
            mkdir -p data/records
            python -m src.harvest_chinaxiv_optimized --month "$MONTH"

            # Verify records file was created
            if [ ! -f "$RECORDS_PATH" ]; then
              echo "‚ùå FATAL: Harvest did not create ${RECORDS_PATH}"
              echo "::endgroup::"
              exit 1
            fi
            echo "   Records file size: $(wc -c < "$RECORDS_PATH" | tr -d ' ') bytes"
            echo "::endgroup::"

            echo "::group::Step 4/6: Downloading PDFs ($(date +%H:%M:%S))"
            echo "üì• Downloading PDFs for ${MISSING_COUNT} missing papers..."
            echo "   Using records: ${RECORDS_PATH}"
            python scripts/download_missing_pdfs.py --months "$MONTH" --paper-ids /tmp/missing_paper_ids.txt
            echo "::endgroup::"

            echo "::group::Step 5/6: Verifying downloads ($(date +%H:%M:%S))"
            DOWNLOADED=0
            STILL_MISSING=0
            while read -r paper_id; do
              if [ -f "data/pdfs/${paper_id}.pdf" ]; then
                DOWNLOADED=$((DOWNLOADED + 1))
              else
                STILL_MISSING=$((STILL_MISSING + 1))
              fi
            done < /tmp/missing_paper_ids.txt

            echo "   Downloaded: ${DOWNLOADED}/${MISSING_COUNT}"
            if [ "$STILL_MISSING" -gt 0 ]; then
              echo "‚ö†Ô∏è  ${STILL_MISSING} papers still missing PDFs after download"
            fi
            echo "::endgroup::"

            echo "::group::Step 6/6: Uploading to B2 ($(date +%H:%M:%S))"
            echo "üì§ Uploading new PDFs to B2..."
            aws s3 sync data/pdfs "${DEST}pdfs/" \
              --exclude "*" \
              --include "chinaxiv-${MONTH}*.pdf" \
              --endpoint-url "${BACKBLAZE_S3_ENDPOINT}" \
              --only-show-errors
            echo "‚úÖ PDFs acquired and uploaded to B2"
            echo "::endgroup::"
          else
            echo "‚úÖ All ${PAPER_COUNT} papers have PDFs in B2"
          fi

          # Count papers with PDFs for figure processing
          PAPERS_WITH_PDFS=0
          while read -r paper_id; do
            [ -z "$paper_id" ] && continue
            if [ -f "data/pdfs/${paper_id}.pdf" ]; then
              PAPERS_WITH_PDFS=$((PAPERS_WITH_PDFS + 1))
            fi
          done < /tmp/paper_ids.txt
          echo "PAPERS_WITH_PDFS=${PAPERS_WITH_PDFS}" >> $GITHUB_ENV

          if [ "${PAPERS_WITH_PDFS}" -eq 0 ]; then
            echo "‚ùå FATAL: No papers have PDFs after acquisition attempt"
            exit 1
          fi
          echo "‚úÖ Will process ${PAPERS_WITH_PDFS} papers with available PDFs ($(date +%H:%M:%S))"

      - name: Run figure pipeline
        if: env.SKIP_FIGURES != 'true'
        env:
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          MOONDREAM_API_KEY: ${{ secrets.MOONDREAM_API_KEY }}
          FIGURE_CONCURRENT: ${{ inputs.figure_concurrent }}
          # B2 credentials for figure upload and status writing
          AWS_ACCESS_KEY_ID: ${{ secrets.BACKBLAZE_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.BACKBLAZE_APPLICATION_KEY }}
          BACKBLAZE_KEY_ID: ${{ secrets.BACKBLAZE_KEY_ID }}
          BACKBLAZE_APPLICATION_KEY: ${{ secrets.BACKBLAZE_APPLICATION_KEY }}
          BACKBLAZE_S3_ENDPOINT: ${{ secrets.BACKBLAZE_S3_ENDPOINT }}
          BACKBLAZE_BUCKET: ${{ secrets.BACKBLAZE_BUCKET }}
          BACKBLAZE_PREFIX: ${{ secrets.BACKBLAZE_PREFIX }}
          # GitHub run info for status writer
          GITHUB_RUN_ID: ${{ github.run_id }}
          GITHUB_REPOSITORY: ${{ github.repository }}
        run: |
          set -e

          echo "üé® Processing figures for month ${{ inputs.month }}..."
          echo "Paper workers: ${{ inputs.workers }}"
          echo "Figure concurrent: ${{ inputs.figure_concurrent }}"
          echo "Limit: ${{ inputs.limit }}"

          # Run the figure pipeline with status writing
          python -c "
          import os
          import sys
          from pathlib import Path

          sys.path.insert(0, str(Path.cwd()))

          from src.figure_pipeline import FigurePipeline, PipelineConfig
          from src import status_writer

          # Get paper IDs from filtered file (or all translations if not filtered)
          limit = int('${{ inputs.limit }}') or None
          workers = int('${{ inputs.workers }}')
          month = '${{ inputs.month }}'

          paper_ids = []

          # Read paper IDs from filter output
          paper_ids_file = Path('/tmp/paper_ids.txt')
          if paper_ids_file.exists():
              with open(paper_ids_file) as f:
                  for line in f:
                      paper_id = line.strip()
                      if paper_id:
                          pdf_path = Path('data/pdfs') / f'{paper_id}.pdf'
                          if pdf_path.exists():
                              paper_ids.append(paper_id)

          if limit:
              paper_ids = paper_ids[:limit]

          print(f'Processing {len(paper_ids)} papers with {workers} workers')

          if not paper_ids:
              print('No papers to process')
              sys.exit(0)

          # Start status tracking
          status_writer.start_stage('figures', total=len(paper_ids), month=month)

          config = PipelineConfig(
              gemini_api_key=os.environ.get('GEMINI_API_KEY'),
              moondream_api_key=os.environ.get('MOONDREAM_API_KEY'),
              pdf_dir='data/pdfs',
              dry_run=False,
          )

          # Process papers in parallel with real-time status updates
          from concurrent.futures import ThreadPoolExecutor, as_completed
          from src.figure_pipeline import FigureProcessingResult

          def process_single(paper_id):
              '''Process single paper, return (paper_id, result, error).'''
              try:
                  p = FigurePipeline(config)  # Fresh instance per thread
                  result = p.process_paper(paper_id)
                  return (paper_id, result, None)
              except Exception as e:
                  return (paper_id, None, str(e))

          results = []
          success = True
          completed_count = 0
          try:
              with ThreadPoolExecutor(max_workers=workers) as executor:
                  futures = {executor.submit(process_single, pid): pid for pid in paper_ids}

                  for future in as_completed(futures):
                      paper_id = futures[future]
                      completed_count += 1
                      pid, result, error = future.result()

                      if error:
                          print(f'[{completed_count}/{len(paper_ids)}] {paper_id}: ERROR - {error}')
                          results.append(FigureProcessingResult(paper_id=paper_id, failed=1))
                          status_writer.record_completion(success=False)
                      else:
                          paper_success = result.translated > 0 or result.total_figures == 0
                          print(f'[{completed_count}/{len(paper_ids)}] {paper_id}: {result.translated}/{result.total_figures} figures')
                          results.append(result)
                          status_writer.record_completion(success=paper_success)

              # Summary
              total_figures = sum(r.total_figures for r in results)
              translated = sum(r.translated for r in results)
              failed = sum(r.failed for r in results)

              print(f'\\n=== SUMMARY ===')
              print(f'Papers processed: {len(results)}')
              print(f'Total figures: {total_figures}')
              print(f'Translated: {translated}')
              print(f'Failed: {failed}')

              if failed > total_figures * 0.5 and total_figures > 0:
                  print('‚ö†Ô∏è High failure rate')
                  success = False

              # Pass actual figures count for inventory (not paper count)
              status_writer.finish_stage(success=success, figures_translated=translated)

              if not success:
                  sys.exit(1)

          except Exception as e:
              print(f'Pipeline failed: {e}')
              status_writer.write_failure(str(e))
              sys.exit(1)
          "

      - name: Upload figures to B2
        if: env.SKIP_FIGURES != 'true'
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.BACKBLAZE_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.BACKBLAZE_APPLICATION_KEY }}
          BACKBLAZE_S3_ENDPOINT: ${{ secrets.BACKBLAZE_S3_ENDPOINT }}
          BACKBLAZE_BUCKET: ${{ secrets.BACKBLAZE_BUCKET }}
          BACKBLAZE_PREFIX: ${{ secrets.BACKBLAZE_PREFIX }}
        run: |
          set -e
          export AWS_DEFAULT_REGION=us-west-004
          DEST="s3://${BACKBLAZE_BUCKET}/${BACKBLAZE_PREFIX}"

          echo "üì§ Uploading figures to B2..."

          # Upload any generated figures
          if [ -d "data/figures" ]; then
            aws s3 sync data/figures "${DEST}figures/" \
              --endpoint-url "${BACKBLAZE_S3_ENDPOINT}" \
              --only-show-errors
            echo "‚úÖ Figures uploaded"
          else
            echo "‚ÑπÔ∏è No figures directory found"
          fi

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: figure-results-${{ inputs.month }}-${{ github.run_id }}
          if-no-files-found: ignore
          retention-days: 30
          path: |
            data/figures/
            reports/figure_*.json
