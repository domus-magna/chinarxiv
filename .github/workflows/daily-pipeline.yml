name: daily-pipeline

on:
  schedule:
    - cron: '0 3 * * *'
  workflow_dispatch:
    inputs:
      skip_harvest:
        description: 'Skip harvest and translation (rebuild only)'
        required: false
        default: false
        type: boolean

concurrency:
  group: daily-pipeline
  cancel-in-progress: true

jobs:
  pipeline:
    runs-on: ubuntu-latest
    permissions:
      contents: write
    steps:
      - name: Capture run metadata
        id: meta
        run: echo "started_at=$(date -u +%Y-%m-%dT%H:%M:%SZ)" >> $GITHUB_OUTPUT
      - name: Set Backblaze region (job-wide)
        run: echo "AWS_DEFAULT_REGION=us-west-004" >> $GITHUB_ENV
      - uses: actions/checkout@v4
        with:
          fetch-depth: 0
          persist-credentials: true
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      - name: Install pandoc for PDF generation
        run: |
          sudo apt-get update
          sudo apt-get install -y pandoc texlive-latex-base texlive-fonts-recommended
          which pandoc || { echo "‚ùå pandoc installation failed"; exit 1; }
          pandoc --version | head -1
      - name: "Preflight: Validate OpenRouter API key"
        env:
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
        if: ${{ github.event_name != 'pull_request' }}
        run: |
          set -e
          echo "üîé Preflight check: validating OPENROUTER_API_KEY"
          python -m src.tools.env_diagnose --check || true
          python -m src.tools.env_diagnose --validate || {
            echo "‚ùå OPENROUTER_API_KEY invalid or missing ‚Äî failing early"
            exit 1
          }
      - name: "Preflight (PR): Non-fatal environment check"
        if: ${{ github.event_name == 'pull_request' }}
        run: |
          set -e
          echo "üîé PR preflight: diagnostics only (no key validation)"
          python -m src.tools.env_diagnose --check || true
      - name: Run lint (pull_request)
        if: ${{ github.event_name == 'pull_request' }}
        run: ruff check src tests
      - name: Run tests (pull_request)
        if: ${{ github.event_name == 'pull_request' }}
        run: python -m pytest tests/ -q
      - name: Run tests (blocking on schedule/manual)
        if: ${{ github.event_name == 'schedule' || (github.event_name == 'workflow_dispatch' && (github.event.inputs.skip_harvest || 'false') != 'true') }}
        run: python -m pytest tests/ -q
      - name: Skip tests (manual rebuild-only)
        if: ${{ github.event_name == 'workflow_dispatch' && (github.event.inputs.skip_harvest || 'false') == 'true' }}
        run: echo "‚ÑπÔ∏è Skipping tests on manual rebuild-only (skip_harvest=true)"
      - name: Run tests (non-blocking on push)
        if: ${{ github.event_name == 'push' }}
        continue-on-error: true
        run: python -m pytest tests/ -q
      - name: Build pipeline (harvest/translate only)
        env:
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
          DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
          BRIGHTDATA_API_KEY: ${{ secrets.BRIGHTDATA_API_KEY }}
          BRIGHTDATA_ZONE: ${{ secrets.BRIGHTDATA_ZONE }}
          # Figure translation (Gemini + Moondream)
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          MOONDREAM_API_KEY: ${{ secrets.MOONDREAM_API_KEY }}
          # Backblaze B2 (S3-compatible)
          BACKBLAZE_KEY_ID: ${{ secrets.BACKBLAZE_KEY_ID }}
          BACKBLAZE_APPLICATION_KEY: ${{ secrets.BACKBLAZE_APPLICATION_KEY }}
          BACKBLAZE_S3_ENDPOINT: ${{ secrets.BACKBLAZE_S3_ENDPOINT }}
          BACKBLAZE_BUCKET: ${{ secrets.BACKBLAZE_BUCKET }}
          BACKBLAZE_PREFIX: ${{ secrets.BACKBLAZE_PREFIX }}
        run: |
          set -e
          echo "üöÄ Starting build process..."

          # Default to skipping harvest during PR builds
          SKIP_HARVEST="${{ github.event_name == 'pull_request' && 'true' || (github.event.inputs.skip_harvest || 'false') }}"
          # Define CURR/PREV unconditionally for metadata/RECORDS_KEYS
          CURR=$(date -u +"%Y%m")
          PREV=$(date -u -d "-1 month" +"%Y%m")

          if [ "$SKIP_HARVEST" = "false" ]; then
            # Attempt ChinaXiv harvest via BrightData if credentials provided
            if [ -n "${BRIGHTDATA_API_KEY}" ] && [ -n "${BRIGHTDATA_ZONE}" ]; then
              echo "üì• Harvesting from ChinaXiv (BrightData)‚Ä¶"
              echo "   Current month: $CURR"
              echo "   Previous month: $PREV"
              python -m src.harvest_chinaxiv_optimized --month "$CURR" || echo "‚ö†Ô∏è Harvest current month failed; continuing"
              python -m src.harvest_chinaxiv_optimized --month "$PREV" || echo "‚ö†Ô∏è Harvest previous month failed; continuing"
            else
              echo "‚ÑπÔ∏è BrightData credentials not set; skipping harvest"
            fi
            echo "üîç Preparing records for selection..."
            python .github/scripts/merge_current_prev_records.py

            echo "üìã Selecting new items (no limit)..."
            if [ -f data/records/_merged_current_prev.json ]; then
              if ! python -m src.select_and_fetch --records data/records/_merged_current_prev.json --output data/selected.json; then
                echo "‚ùå Record selection failed"
                python -m src.tools.b2_alerts add "record selection failed"
                python -m src.tools.b2_alerts flush || true
                exit 1
              fi
            else
              echo "‚ö†Ô∏è No records found to select"
              echo '[]' > data/selected.json
            fi

            if [ ! -f data/selected.json ]; then
              echo "‚ùå selection file missing after select_and_fetch"
              python -m src.tools.b2_alerts add "selection output missing"
              python -m src.tools.b2_alerts flush || true
              exit 1
            fi

            SELECTION_COUNT=$(python3 .github/scripts/count_json_array.py data/selected.json)
            if [ "$SELECTION_COUNT" -lt 0 ]; then
              echo "‚ùå Unable to read selection output"
              python -m src.tools.b2_alerts add "selection unreadable"
              python -m src.tools.b2_alerts flush || true
              exit 1
            fi
            echo "SELECTION_COUNT=$SELECTION_COUNT" >> $GITHUB_ENV

          # Persist selection to B2 (dated + latest), then strictly read from B2 (skip on PRs)
          echo "üß≠ Preparing selection in Backblaze B2..."
          CI_IS_PR="${{ github.event_name == 'pull_request' && 'true' || 'false' }}"
          if [ -n "${BACKBLAZE_KEY_ID}" ] && [ -n "${BACKBLAZE_APPLICATION_KEY}" ] && [ -n "${BACKBLAZE_S3_ENDPOINT}" ] && [ -n "${BACKBLAZE_BUCKET}" ]; then
            python -m pip install --upgrade pip >/dev/null 2>&1 || true
            pip install awscli >/dev/null 2>&1 || true
            export AWS_ACCESS_KEY_ID="${BACKBLAZE_KEY_ID}"
            export AWS_SECRET_ACCESS_KEY="${BACKBLAZE_APPLICATION_KEY}"
            export AWS_DEFAULT_REGION=us-west-004
              DEST="s3://${BACKBLAZE_BUCKET}/${BACKBLAZE_PREFIX}"
              DAY=$(date -u +"%Y-%m-%d")
              SELECT_KEY="selections/daily/${DAY}.json"
              aws s3 cp data/selected.json "${DEST}${SELECT_KEY}" --endpoint-url "${BACKBLAZE_S3_ENDPOINT}" || {
                echo "‚ùå Failed to upload selection to B2";
                python -m src.tools.b2_alerts add "upload selection failed: ${SELECT_KEY}";
                exit 1;
              }
              # Write latest pointer JSON
              echo "{\"path\": \"${SELECT_KEY}\", \"run_id\": \"${GITHUB_RUN_ID}\", \"ts\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\"}" > latest.json
              aws s3 cp latest.json "${DEST}selections/latest.json" --endpoint-url "${BACKBLAZE_S3_ENDPOINT}" || {
                echo "‚ùå Failed to update selections/latest.json";
                python -m src.tools.b2_alerts add "update latest.json failed";
                exit 1;
              }
              # Strictly read back from B2 to ensure CI references B2, not local
              rm -f data/selected.json
              aws s3 cp "${DEST}${SELECT_KEY}" data/selected.json --endpoint-url "${BACKBLAZE_S3_ENDPOINT}" || {
                echo "‚ùå Failed to pull selection from B2";
                python -m src.tools.b2_alerts add "download selection failed: ${SELECT_KEY}";
                exit 1;
              }
              export SELECT_KEY
              echo "SELECT_KEY=${SELECT_KEY}" >> $GITHUB_ENV
              echo "RECORDS_KEYS=records/chinaxiv_${CURR}.json;records/chinaxiv_${PREV}.json" >> $GITHUB_ENV
            else
              if [ "$CI_IS_PR" = "true" ]; then
                echo "‚ÑπÔ∏è PR build: skipping B2 selection persist (secrets withheld). Using local data/selected.json."
              else
                echo "‚ùå B2 secrets missing; cannot persist selection"
                python -m src.tools.b2_alerts add "B2 selection persist skipped: missing secrets"
                exit 1
              fi
            fi

            # MANDATORY: Download PDFs before translation
            echo "üì• Downloading PDFs for all selected papers..."
            python scripts/download_missing_pdfs.py || {
              echo "‚ö†Ô∏è Some PDF downloads failed, continuing with available PDFs"
            }

            # Upload PDFs to B2 BEFORE translation (ensures persistence)
            if [ -d data/pdfs ] && ls data/pdfs/*.pdf 1>/dev/null 2>&1; then
              echo "üì§ Uploading PDFs to B2 before translation..."
              aws s3 sync data/pdfs "${DEST}pdfs/" \
                --exclude "*" \
                --include "chinaxiv-*.pdf" \
                --endpoint-url "${BACKBLAZE_S3_ENDPOINT}" \
                --only-show-errors
              PDF_COUNT=$(ls -1 data/pdfs/*.pdf 2>/dev/null | wc -l | tr -d ' ')
              echo "‚úÖ ${PDF_COUNT} PDFs synced to B2"
            fi

            # Translate (text + figures for non-PR builds)
            echo "üåê Running translation pipeline..."
            FIGURE_FLAG=""
            if [ "${{ github.event_name }}" != "pull_request" ]; then
              FIGURE_FLAG="--with-figures"
            fi
            python -m src.pipeline --skip-selection --workers 20 --with-qa $FIGURE_FLAG

            SUMMARY_FILE="reports/pipeline_summary.json"
            if [ ! -f "$SUMMARY_FILE" ]; then
              echo "‚ùå Missing pipeline summary"
              python -m src.tools.b2_alerts add "pipeline summary missing"
              python -m src.tools.b2_alerts flush || true
              exit 1
            fi

            PIPELINE_SUCCESS_COUNT=$(python3 .github/scripts/read_pipeline_summary.py successes)
            PIPELINE_QA_PASSED=$(python3 .github/scripts/read_pipeline_summary.py qa_passed)
            echo "PIPELINE_SUCCESS_COUNT=$PIPELINE_SUCCESS_COUNT" >> $GITHUB_ENV
            echo "PIPELINE_QA_PASSED=$PIPELINE_QA_PASSED" >> $GITHUB_ENV
            echo "B2_PUBLISH_SUMMARY_PATH=reports/b2_publish_summary.json" >> $GITHUB_ENV

            if [ "$SKIP_HARVEST" = "false" ] && [ "$PIPELINE_SUCCESS_COUNT" -eq 0 ]; then
              if [ "${TRANSLATION_OPTIONAL:-false}" = "true" ]; then
                echo "‚ö†Ô∏è Pipeline produced zero translations but TRANSLATION_OPTIONAL is true; continuing"
              else
                echo "‚ùå Pipeline produced zero translations; failing build"
                python -m src.tools.b2_alerts add "pipeline produced zero translations"
                python -m src.tools.b2_alerts flush || true
                exit 1
              fi
            fi
          else
            echo "‚ÑπÔ∏è Skipping harvest and translation (rebuild only)"
          fi
          
          echo "‚ÑπÔ∏è Deferring render/index/pdf to post-hydration steps"

      - name: Publish validated/flagged/PDFs to B2 and write manifests
        if: ${{ github.event_name != 'pull_request' }}
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.BACKBLAZE_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.BACKBLAZE_APPLICATION_KEY }}
          B2_S3_ENDPOINT: ${{ secrets.BACKBLAZE_S3_ENDPOINT }}
          B2_BUCKET: ${{ secrets.BACKBLAZE_BUCKET }}
          B2_PREFIX: ${{ secrets.BACKBLAZE_PREFIX }}
          SELECT_KEY: ${{ env.SELECT_KEY }}
          RECORDS_KEYS: ${{ env.RECORDS_KEYS }}
          RUN_STARTED_AT: ${{ steps.meta.outputs.started_at }}
          B2_FAIL_ON_ERROR: 'true'
        run: |
          # Ensure AWS CLI signs requests for Backblaze region
          export AWS_DEFAULT_REGION=us-west-004
          python -m pip install --upgrade pip >/dev/null 2>&1 || true
          pip install awscli >/dev/null 2>&1 || true
          python -m src.tools.b2_publish || {
            echo "‚ùå B2 publish failed";
            exit 1;
          }
          if [ ! -f "${B2_PUBLISH_SUMMARY_PATH:-reports/b2_publish_summary.json}" ]; then
            echo "‚ùå Missing B2 publish summary"
            python -m src.tools.b2_alerts add "b2 publish summary missing"
            python -m src.tools.b2_alerts flush || true
            exit 1
          fi

      - name: Hydrate validated translations from B2 (pre-render)
        if: ${{ github.event_name != 'pull_request' }}
        env:
          BACKBLAZE_KEY_ID: ${{ secrets.BACKBLAZE_KEY_ID }}
          BACKBLAZE_APPLICATION_KEY: ${{ secrets.BACKBLAZE_APPLICATION_KEY }}
          BACKBLAZE_S3_ENDPOINT: ${{ secrets.BACKBLAZE_S3_ENDPOINT }}
          BACKBLAZE_BUCKET: ${{ secrets.BACKBLAZE_BUCKET }}
          BACKBLAZE_PREFIX: ${{ secrets.BACKBLAZE_PREFIX }}
          HYDRATE_EXPECTED_COUNT_FILE: ${{ env.B2_PUBLISH_SUMMARY_PATH }}
        run: |
          set -e
          if [ -z "${BACKBLAZE_KEY_ID}" ] || [ -z "${BACKBLAZE_APPLICATION_KEY}" ] || [ -z "${BACKBLAZE_S3_ENDPOINT}" ] || [ -z "${BACKBLAZE_BUCKET}" ]; then
            echo "‚ùå Missing B2 credentials for hydration";
            python -m src.tools.b2_alerts add "hydrate skipped: missing B2 credentials";
            exit 1;
          fi
          python -m pip install --upgrade pip >/dev/null 2>&1 || true
          pip install awscli >/dev/null 2>&1 || true
          if ! python scripts/hydrate_from_b2.py --target data/translated; then
            python -m src.tools.b2_alerts add "hydrate failed";
            python -m src.tools.b2_alerts flush || true
            exit 1;
          fi

          # Download figure manifest for translated figure display
          export AWS_ACCESS_KEY_ID="${BACKBLAZE_KEY_ID}"
          export AWS_SECRET_ACCESS_KEY="${BACKBLAZE_APPLICATION_KEY}"
          export AWS_DEFAULT_REGION=us-west-004
          DEST="s3://${BACKBLAZE_BUCKET}/${BACKBLAZE_PREFIX}"
          echo "üì• Downloading figure manifest from B2..."
          mkdir -p data
          aws s3 cp "${DEST}figures/manifest.json" data/figure_manifest.json \
            --endpoint-url "${BACKBLAZE_S3_ENDPOINT}" \
            --only-show-errors || echo "‚ÑπÔ∏è No figure manifest found (figures not yet translated)"

      - name: Render site (post-hydration)
        run: |
          set -e
          echo "üé® Rendering site from hydrated data..."
          # PDF generation is now integrated into render.py
          python -m src.render
          echo "üîç Building search index..."
          python -m src.search_index
          # Count generated PDFs
          PDF_COUNT=$(find site/items -name "*.pdf" 2>/dev/null | wc -l | tr -d ' ')
          echo "üìÑ Generated ${PDF_COUNT} English PDFs"
      - name: Publish run summary
        if: ${{ github.event_name != 'pull_request' }}
        env:
          BACKBLAZE_KEY_ID: ${{ secrets.BACKBLAZE_KEY_ID }}
          BACKBLAZE_APPLICATION_KEY: ${{ secrets.BACKBLAZE_APPLICATION_KEY }}
          BACKBLAZE_S3_ENDPOINT: ${{ secrets.BACKBLAZE_S3_ENDPOINT }}
          BACKBLAZE_BUCKET: ${{ secrets.BACKBLAZE_BUCKET }}
          BACKBLAZE_PREFIX: ${{ secrets.BACKBLAZE_PREFIX }}
          DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
        run: |
          set -e
          python scripts/publish_run_summary.py
      - name: Set up Node.js
        if: ${{ github.event_name != 'pull_request' }}
        uses: actions/setup-node@v4
        with:
          node-version: '20'
      - name: Deploy to Cloudflare Pages
        if: ${{ github.event_name != 'pull_request' }}
        env:
          CLOUDFLARE_API_TOKEN: ${{ secrets.CF_API_TOKEN }}
        run: |
          set -e
          echo "üöÄ Deploying to Cloudflare Pages..."
          
          echo "üì¶ Installing Wrangler CLI..."
          npm install -g wrangler || {
            echo "‚ùå Failed to install Wrangler"
            exit 1
          }
          
          echo "üåê Deploying site..."
          wrangler pages deploy site --project-name chinarxiv || {
            echo "‚ùå Deployment failed"
            exit 1
          }
          
          echo "‚úÖ Deployment completed successfully!"
      - name: Persist pipeline outputs to Backblaze B2 (JSON only)
        if: ${{ github.event_name != 'pull_request' }}
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.BACKBLAZE_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.BACKBLAZE_APPLICATION_KEY }}
          B2_S3_ENDPOINT: ${{ secrets.BACKBLAZE_S3_ENDPOINT }}
          B2_BUCKET: ${{ secrets.BACKBLAZE_BUCKET }}
          B2_PREFIX: ${{ secrets.BACKBLAZE_PREFIX }}
        run: |
          set -e
          # B2 persistence is MANDATORY - fail if secrets are missing
          if [ -z "${AWS_ACCESS_KEY_ID}" ] || [ -z "${AWS_SECRET_ACCESS_KEY}" ] || [ -z "${B2_S3_ENDPOINT}" ] || [ -z "${B2_BUCKET}" ]; then
            echo "‚ùå B2 secrets missing - persistence is MANDATORY"
            python -m src.tools.b2_alerts add "B2 persistence failed: missing secrets"
            exit 1
          fi
          python -m pip install --upgrade pip
          pip install awscli
          export AWS_DEFAULT_REGION=us-west-004
          DEST="s3://${B2_BUCKET}/${B2_PREFIX}"
          echo "üì§ Persisting JSON outputs to ${DEST} via ${B2_S3_ENDPOINT}"
          if [ -d data/records ]; then
            aws s3 cp data/records "${DEST}records/" --recursive --exclude "*" --include "*.json" --endpoint-url "${B2_S3_ENDPOINT}" --only-show-errors
          fi
          if [ -d data/translated ]; then
            aws s3 cp data/translated "${DEST}translations/" --recursive --exclude "*" --include "*.json" --endpoint-url "${B2_S3_ENDPOINT}" --only-show-errors
          fi
          if [ -f data/selected.json ]; then
            aws s3 cp data/selected.json "${DEST}selections/${GITHUB_RUN_ID}/selected.json" --endpoint-url "${B2_S3_ENDPOINT}" --only-show-errors
          fi
      - name: Flush B2 alert buffer (if any)
        if: ${{ github.event_name != 'pull_request' }}
        run: |
          python -m src.tools.b2_alerts flush || true
      - name: Upload harvest artifacts (records + selection)
        if: ${{ github.event_name != 'pull_request' }}
        uses: actions/upload-artifact@v4
        with:
          name: harvest-data-${{ github.run_id }}
          if-no-files-found: ignore
          retention-days: 90
          path: |
            data/records/*.json
            data/selected.json
      - name: Persist dedupe state (seen.json)
        if: ${{ github.event_name != 'pull_request' }}
        run: |
          set -e
          if [ ! -f data/seen.json ]; then
            echo "No seen.json to persist; skipping"
            exit 0
          fi
          echo "üîí Persisting data/seen.json..."
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git add data/seen.json || true
          if git diff --cached --quiet; then
            echo "No changes in seen.json"
          else
            git commit -m "chore(dedupe): update seen.json [skip ci]" || true
            git push || echo "‚ö†Ô∏è Push failed (non-fatal)"
          fi
